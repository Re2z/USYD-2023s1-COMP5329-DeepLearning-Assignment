{"cells":[{"cell_type":"markdown","metadata":{"id":"WIPFQwUlrYbf"},"source":["# COMP5329-Assignment1"]},{"cell_type":"markdown","source":["## Connect to the Colab and download the dataset"],"metadata":{"id":"UDEc4PAP0R18"}},{"cell_type":"markdown","source":["This block is used to download the dataset in the shared folder to your own google drive in order to load the dataset later."],"metadata":{"id":"AE4MAepqA8wq"}},{"cell_type":"code","source":["from pydrive.auth import GoogleAuth\n","from google.colab import drive\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"metadata":{"id":"bi6GaGj8xXTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","test_data = '1nTI0MVAUcwc1OQXjtdM8ffprx6Tilil3' #<-- You add in here the id from you google drive file, you can find it\n","test_label = '1elmJT3LyMJ9i8mU1-WPsrZzcqg__lEwQ'\n","train_data = '1R9gZfVmCLJgSml9D86PkwASiti4l1rDA'\n","train_label = '1UwisZaReXDwjAcRQCeZ5vMs3--MgnEhf'\n","\n","download_test_data = drive.CreateFile({'id': test_data})\n","download_test_label = drive.CreateFile({'id': test_label})\n","download_train_data = drive.CreateFile({'id': train_data})\n","download_train_label = drive.CreateFile({'id': train_label})\n","\n","\n","# Download the file to a local disc\n","download_test_data.GetContentFile('test_data.npy')\n","download_test_label.GetContentFile('test_label.npy')\n","download_train_data.GetContentFile('train_data.npy')\n","download_train_label.GetContentFile('train_label.npy')"],"metadata":{"id":"-7cty5x0yEu_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"11a9P4AxrZLI"},"source":["## Loading the packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffNyUbFrr4yZ"},"outputs":[],"source":["import math\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"]},{"cell_type":"markdown","metadata":{"id":"jpXbmEm84ugV"},"source":["## Start time"]},{"cell_type":"markdown","source":["Get the code start time to calculate the run time."],"metadata":{"id":"mZJ8ZLcdBRGO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"z36EUWyV4zZN"},"outputs":[],"source":["# Obtain the start time\n","start = time.time()"]},{"cell_type":"markdown","metadata":{"id":"-sqKpkMHsBiP"},"source":["## The Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XP_WioG2sGjX"},"outputs":[],"source":["# Load the dataset\n","X = np.load(\"train_data.npy\")\n","y = np.load(\"train_label.npy\")\n","X_test = np.load(\"test_data.npy\")\n","y_test = np.load(\"test_label.npy\")\n","\n","# Split the dataset to the training set and validation set, the proportion of is 8:1:1\n","X_train = X[:40000]\n","y_train = y[:40000]\n","X_val = X[40000:]\n","y_val = y[40000:]"]},{"cell_type":"markdown","metadata":{"id":"GmmNKnJbuOLC"},"source":["## Definition of activation functions"]},{"cell_type":"markdown","metadata":{"id":"pieGiJhYuZZ8"},"source":["RuLU\n","$$output = max(0,x)$$\n","\n","GeLU\n","$$output = 0.5x(1+tanh(\\sqrt{2/π}(x+0.044715x^3)))$$  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqXB9zqduQaW"},"outputs":[],"source":["class Activation(object):\n","    \"\"\"\n","    Create an activation class\n","    For each time, we can initialize an activation function object with one specific function\n","    For example: f = Activation(\"relu\")  means we create a ReLU activation function.\n","\n","    Define the two activation functions: ReLU and GeLU\n","    With their derivative functions\n","    \"\"\"\n","\n","    # ReLU activation\n","    def __relu(self, x):\n","        return np.maximum(0, x)\n","\n","    def __relu_derive(self, a):\n","        # a = relu(x)\n","        return (a > 0) * 1\n","    \n","    \"\"\"--------------------------Advanced Module-----GELU--------------------\"\"\"\n","    # GELU activation. Advanced Module\n","    def __gelu(self, x):\n","        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n","\n","    def __gelu_derive(self, a):\n","        # a = gelu(x)\n","        return ((np.tanh((np.sqrt(2) * (0.044715 * a ** 3 + a)) / np.sqrt(np.pi)) + ((np.sqrt(2) * a * (\n","                0.134145 * a ** 2 + 1) * ((1 / np.cosh(\n","            (np.sqrt(2) * (0.044715 * a ** 3 + a)) / np.sqrt(np.pi))) ** 2)) / np.sqrt(np.pi) + 1))) / 2\n","\n","    def __init__(self, activation='relu'):\n","        if activation == 'relu':\n","            self.f = self.__relu\n","            self.f_deriv = self.__relu_derive\n","        elif activation == 'gelu':\n","            self.f = self.__gelu\n","            self.f_deriv = self.__gelu_derive"]},{"cell_type":"markdown","metadata":{"id":"By9c-o3hxtKb"},"source":["## Define HiddenLayer"]},{"cell_type":"markdown","metadata":{"id":"1Jx1nonn2AMQ"},"source":["\n","$$output = f\\_act(f\\_batchnorm(f\\_dropout(\\sum_{i=0}^{1}{(I_{i} * W_{i})} + b)))$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOTltG0Fxuun"},"outputs":[],"source":["class HiddenLayer(object):\n","    def __init__(self, n_in, n_out, activation_last_layer='relu', activation='relu'):\n","        \"\"\"\n","        Define the hidden layer for the mlp. For example, h1 = HiddenLayer(10, 5, activation=\"relu\")\n","        Means we create a layer with 10 dimension input and 5 dimension output, and using tanh activation function.\n","        Make sure the input size of hidden layer should be matched with the output size of the previous layer!\n","\n","        Typical hidden layer of an MLP: units are fully-connected and have\n","        ReLU activation function, batch normalization and dropout.\n","        Weight matrix W is of shape (n_in,n_out) and the bias vector b is of shape (n_out,).\n","\n","        NOTE : The nonlinear used here is ReLU\n","\n","        Hidden unit activation is given by: ReLU(dot(input,W) + b)\n","\n","        :type n_in: int\n","        :param n_in: dimensionality of input\n","\n","        :type n_out: int\n","        :param n_out: number of hidden units\n","\n","        :type activation: string\n","        :param activation: non-linearity to be applied in the hidden layer\n","        \"\"\"\n","        self.input = None\n","        self.output = None\n","        if activation:\n","            self.activation = Activation(activation).f\n","\n","        # Activation derivative of last layer\n","        self.activation_deriv = None\n","        if activation_last_layer:\n","            self.activation_deriv = Activation(activation_last_layer).f_deriv\n","\n","        # Randomly assign small values. Because the ReLU and GeLU are both non-linear activation\n","        # So, we can use the HE initialization for them.\n","        self.W = np.random.uniform(low=-np.sqrt(2 / n_out), high=np.sqrt(2 / n_out), size=(n_in, n_out))\n","        self.b = np.zeros(n_out, )\n","\n","        # We set he size of weight gradation as the size of weight\n","        self.grad_W = np.zeros(self.W.shape)\n","        self.grad_b = np.zeros(self.b.shape)\n","\n","        # Dropout parameter for:\n","        # dropout: whether to use dropout. mask: dropout matrix. output_layer: whether the layer is the output layer.\n","        self.dropout = None\n","        self.mask = None\n","        self.output_layer = False\n","\n","        # The parameter for the Batch normalization, including:\n","        # Whether to use the batch normalization. The mean and variance in testing process\n","        # The gamma and bete with their update parameters.\n","        self.batch_norm = None\n","        self.running_mean = None\n","        self.running_var = None\n","        self.std = None\n","        self.xn = None\n","        self.xc = None\n","        self.batch_size = None\n","        self.gamma = np.ones(n_out, )\n","        self.beta = np.zeros(n_out, )\n","        self.grad_gamma = np.zeros(self.gamma.shape)\n","        self.grad_beta = np.zeros(self.beta.shape)\n","\n","        # The parameter for Momentum batch normalization\n","        # Indicates the current velocity of the weight and bias momentum\n","        self.grad_W_V = np.zeros(self.W.shape)\n","        self.grad_b_V = np.zeros(self.b.shape)\n","        self.grad_gamma_V = np.zeros(self.gamma.shape)\n","        self.grad_beta_V = np.zeros(self.beta.shape)\n","\n","        # Adam in batch normalization\n","        # The 1st moment vector\n","        self.grad_W_mt = np.zeros(self.W.shape)\n","        self.grad_b_mt = np.zeros(self.b.shape)\n","        self.grad_gamma_mt = np.zeros(self.gamma.shape)\n","        self.grad_beta_mt = np.zeros(self.beta.shape)\n","        # The 2nd moment vector (squared)\n","        self.grad_W_vt = np.zeros(self.W.shape)\n","        self.grad_b_vt = np.zeros(self.b.shape)\n","        self.grad_gamma_vt = np.zeros(self.gamma.shape)\n","        self.grad_beta_vt = np.zeros(self.beta.shape)\n","\n","    def forward(self, input, test_flg=False, dropout_rate=0.5):\n","        \"\"\"\n","        The forward progress for the hidden layer, including the dropout and batch normalization\n","        Calculate the output of the output layer\n","        Can determine whether to use the test mode\n","\n","        :type input: numpy.array\n","        :param input: a symbolic tensor of shape (batch_size, n_in)\n","\n","        :type test_flg: boolean\n","        :param test_flg: whether is the test progress\n","\n","        :type dropout_rate: float\n","        :param dropout_rate: probability for dropout\n","\n","        :return: output of shape (batch_size, n_out) for this hidden layer\n","        \"\"\"\n","        # Calculate the perception\n","        lin_output = np.dot(input, self.W) + self.b\n","\n","        # For the forward progress, use dropout first and then batch normalization\n","        # The dropout progress\n","        if self.dropout == True and self.output_layer == False:\n","            lin_output = self.dropout_forward(lin_output, dropout_rate=dropout_rate, test_flg=test_flg)\n","\n","        # The batch normalization progress\n","        if self.batch_norm:\n","            lin_output = self.batch_normalization_forward(lin_output, test_flg=test_flg)\n","\n","        # Use the activation function\n","        self.output = (\n","            lin_output if self.activation is None\n","            else self.activation(lin_output)\n","        )\n","        self.input = input\n","        return self.output\n","\n","    def backward(self, delta, output_layer=False):\n","        \"\"\"\n","        The backward progress for the hidden layer, including the dropout and batch normalization\n","        Calculate the delta for the derivative used for calculating loss function\n","        \n","        :type delta: numpy.array\n","        :param delta: a symbolic tensor of shape (batch_size, n_out)\n","        \n","        :type output_layer: boolean\n","        :param output_layer: whether is the output layer of the network or not\n","         \n","        :return: new delta of shape (batch_size, n_in)\n","        \"\"\"\n","        self.output_layer = output_layer\n","        # For the backward progress, consider batch normalization first and then dropout\n","        # The batch normalization backward progress\n","        if self.batch_norm:\n","            delta = self.batch_normalization_backward(delta)\n","\n","        # The dropout backward progress\n","        if self.dropout == True and self.output_layer == False:\n","            delta = self.dropout_backward(delta)\n","\n","        # Calculate the gradation of weight and bias for update\n","        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n","        self.grad_b = np.sum(delta, axis=0)\n","\n","        # The activation derivative progress\n","        if self.activation_deriv:\n","            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n","        return delta\n","\n","    def batch_normalization_forward(self, x, test_flg=False):\n","        \"\"\"\n","        The batch normalization forward progress, including the train mode and the test mode\n","        Used for normalization for the input\n","        Accept the output of the perception and return a new output\n","\n","        :type x: numpy.array\n","        :param x: a symbolic tensor of shape (batch_size, n_out)\n","\n","        :type test_flg: boolean\n","        :param test_flg: determine the test mode or train mode\n","\n","        :return: output of batch normalization in shape (batch_size, n_out)\n","        \"\"\"\n","        # Initialize the mean and variance of the input with the shape of (,n_in)\n","        if self.running_mean is None:\n","            # self.input_shape = x.shape\n","            N, D = x.shape\n","            self.running_mean = np.zeros(D)\n","            self.running_var = np.zeros(D)\n","\n","        # If it is the train progress\n","        if test_flg is False:\n","            # Calculate the mean\n","            mean = np.mean(x, axis=0)\n","            # Subtract the mean of each training example\n","            xc = x - mean\n","            # Calculate the variance\n","            var = np.mean(xc ** 2, axis=0)\n","            # Add epsilon for numerical stability, then sqrt\n","            std = np.sqrt(var + 10e-7)\n","            # Execute normalization\n","            xn = xc / std\n","\n","            # Store the parameters\n","            self.batch_size = x.shape[0]\n","            self.xc = xc\n","            self.xn = xn\n","            self.std = std\n","            # Update the mean and variance after each progress\n","            self.running_mean = 0.9 * self.running_mean + (1 - 0.9) * mean\n","            self.running_var = 0.9 * self.running_var + (1 - 0.9) * var\n","        else:\n","            # In the test progress of batch normalization, only two steps\n","            # Subtract the mean of each test example\n","            xc = x - self.running_mean\n","            # Execute normalization\n","            xn = xc / (np.sqrt(self.running_var + 10e-7))\n","\n","        # The transformation step\n","        out = self.gamma * xn + self.beta\n","        # return out.reshape(*self.input_shape)\n","        return out\n","\n","    def batch_normalization_backward(self, dout):\n","        \"\"\"\n","        The backward progress for the batch normalization.\n","        Accept the delta and return a new\n","\n","        :type dout: numpy.array\n","        :param dout: a symbolic tensor of shape (batch_size, n_out)\n","\n","        :return: new delta of shape (batch_size, n_out)\n","        \"\"\"\n","        # Calculate the derivative of the beta\n","        dbeta = np.sum(dout, axis=0)\n","        # Calculate the derivative of the gamma\n","        dgamma = np.sum(dout * self.xn, axis=0)\n","        # Derivative of the batch normalization forward progress\n","        dxn = self.gamma * dout\n","        dxc = dxn / self.std\n","        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n","        dvar = 0.5 * dstd / self.std\n","        dxc += (2.0 / self.batch_size) * self.xc * dvar\n","        dmu = np.sum(dxc, axis=0)\n","        # Calculate the new delta\n","        dx = dxc - dmu / self.batch_size\n","\n","        # Update the parameters\n","        self.grad_gamma = dgamma\n","        self.grad_beta = dbeta\n","        # return dx.reshape(*self.input_shape)\n","        return dx\n","\n","    def dropout_forward(self, x, dropout_rate=0.5, test_flg=False):\n","        \"\"\"\n","        The dropout forward progress, dropout the several nodes in the neural network of each hidden layer\n","\n","        :type x: numpy.array\n","        :param x: a symbolic tensor of shape (batch_size, n_out)\n","\n","        :type dropout_rate: float\n","        :param dropout_rate: rate for the dropout\n","\n","        :type test_flg: boolean\n","        :param test_flg: determine the test mode or train mode\n","\n","        :return: output of dropout in shape of (batch_size, n_out)\n","        \"\"\"\n","        # If it is the train progress\n","        if not test_flg:\n","            # Create a matrix in binary (0 and 1) according to the dropout rate and rescale\n","            self.mask = np.random.binomial(1, 1 - dropout_rate, x.shape) / (1 - dropout_rate)\n","            return x * self.mask\n","        else:\n","            return x\n","\n","    def dropout_backward(self, dout):\n","        \"\"\"\n","        The dropout backward progress\n","\n","        :type dout: numpy.array\n","        :param dout: a symbolic tensor of shape (batch_size, n_out)\n","\n","        :return: new delta of shape (batch_size, n_out)\n","        \"\"\"\n","        if self.mask is None:\n","            return dout\n","        else:\n","            return dout * self.mask"]},{"cell_type":"markdown","metadata":{"id":"SdaXnEzw21zv"},"source":["## The MLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-AgSWXn26AN"},"outputs":[],"source":["class MLP:\n","    def __init__(self, layers, activation=[None, 'relu', 'relu'], dropout=True, batch_norm=True):\n","        \"\"\"\n","        For initialization, the code will create all layers automatically based on the provided parameters.\n","        The Multi-Layer perception, also known as the Artificial Neural Network\n","        Basic including the input layer, hidden layer and the output layer\n","        In this MLP, we add the batch normalization, softmax with cross entropy and activation layer\n","        User can choose the two types od optimizer for the parameter update: momentum in SGD and Adam\n","        The data training is in form of the mini-batch\n","\n","        :type layers: list\n","        :param layers: a list containing the number of units in each layer.\n","        Should be at least two values\n","\n","        :type activation: list\n","        :param activation: activation function to be used. Can be \"logistic\" or \"tanh\"\n","\n","        :type dropout: boolean\n","        :param dropout: whether to use the dropout or not\n","\n","        :type batch_norm: boolean\n","        :param batch_norm: whether to use the batch normalization or not\n","        \"\"\"\n","        # Initialize layers\n","        self.layers = []\n","        self.params = []\n","\n","        # Append the hidden layer and activation layer to the initialization\n","        self.activation = activation\n","        for i in range(len(layers) - 1):\n","            self.layers.append(HiddenLayer(layers[i], layers[i + 1], activation[i], activation[i + 1]))\n","\n","        # Set the last layer as the output layer\n","        self.layers[-1].output_layer = True\n","\n","        # Set the attributes of whether to use dropout and batch normalization or not\n","        for layer in self.layers:\n","            layer.dropout = dropout\n","            layer.batch_norm = batch_norm\n","\n","    def forward(self, input, dropout_rate=0.5, test_flg=False):\n","        \"\"\"\n","        Forward progress: pass the information through the layers and out the results of final output layer\n","\n","        :type input: numpy.array\n","        :param input: a symbolic tensor of shape (batch_size, n_in)\n","\n","        :type dropout_rate: float\n","        :param dropout_rate: probability for dropout\n","\n","        :type test_flg: boolean\n","        :param test_flg: whether is the test progress\n","\n","        :return: output of shape (batch_size, n_out) for this hidden layer\n","        \"\"\"\n","        for layer in self.layers:\n","            output = layer.forward(input, dropout_rate=dropout_rate, test_flg=test_flg)\n","            input = output\n","        return output\n","\n","    def crossEntropy_softmax(self, y_hat, y):\n","        \"\"\"\n","        The softmax with cross entropy layer accepting two parameters: y_predict and y_target\n","        Combine the softmax with the cross entropy to calculate the loss\n","\n","        :type y_hat: numpy.array\n","        :param y_hat: predict value of the input data\n","\n","        :type y: numpy.array\n","        :param y: target value of the input data\n","\n","        :return: loss of the output\n","        \"\"\"\n","        # Calculate the softmax\n","        exps = np.exp(y_hat - np.max(y_hat, axis=1, keepdims=True))\n","        out = exps / np.sum(exps, axis=1, keepdims=True)\n","        batch_size = y.shape[0]\n","        # Calculate the cross entropy loss\n","        return -np.sum(np.log(out[np.arange(batch_size), y.reshape(-1)] + 1e-7)) / batch_size\n","\n","    def delta_crossEntropy_softmax(self, y_hat, y):\n","        \"\"\"\n","        The softmax with cross entropy layer accepting two parameters: y_predict and y_target\n","        Combine the softmax with the cross entropy to calculate the delta\n","\n","        :type y_hat: numpy.array\n","        :param y_hat: predict value of the input data\n","\n","        :type y: numpy.array\n","        :param y: target value of the input data\n","\n","        :return: the delta of the output\n","        \"\"\"\n","        # Calculate the softmax\n","        exps = np.exp(y_hat - np.max(y_hat, axis=1, keepdims=True))\n","        out = exps / np.sum(exps, axis=1, keepdims=True)\n","        batch_size = y.shape[0]\n","        # Calculate the cross entropy delta\n","        # Because the shape of y is (batch_size,1), we need to transfer it to the (batch_size,)\n","        out[np.arange(batch_size), y.reshape(-1)] -= 1\n","        return out / batch_size\n","\n","    def criterion_CE(self, y, y_hat):\n","        \"\"\"\n","        Define the objection/loss function, we use Cross-Entropy as the loss\n","        Using the cross entropy function to calculate the loss and delta\n","        Including the derivative of the activation function\n","\n","        :type y: numpy.array\n","        :param y: target value of the input data\n","\n","        :type y_hat: numpy.array\n","        :param y_hat: predict value of the input data\n","\n","        :return: loss and delta\n","        \"\"\"\n","        activation_deriv = Activation(self.activation[-1]).f_deriv\n","        # Cross Entropy delta and loss\n","        error = self.delta_crossEntropy_softmax(y_hat, y)\n","        loss = self.crossEntropy_softmax(y_hat, y)\n","        delta = error * activation_deriv(y_hat)\n","        return loss, delta\n","\n","    def backward(self, delta):\n","        \"\"\"\n","        Backward progress: pass the delta and update the parameter for next forward progress\n","\n","        :type delta: numpy.array\n","        :param delta: a symbolic tensor of shape (batch_size, n_out)\n","        \"\"\"\n","        # Set the last layer as the output layer\n","        delta = self.layers[-1].backward(delta, output_layer=True)\n","        for layer in reversed(self.layers[:-1]):\n","            delta = layer.backward(delta)\n","\n","    def update(self, lr, optimizer=None, weight_decay_lambda=1, momentum=0.9, iter=100, rho1=0.9, rho2=0.999,\n","               epsilon=1e-8):\n","        \"\"\"\n","        Update the network weights after backward.\n","        The update for the weight and bias, lr decide the learning rate\n","        of the weight update. None, Momentum and Adam can be selected for\n","        the optimizer.\n","\n","        :type lr: float\n","        :param lr: learning rate for the update\n","\n","        :type optimizer: str\n","        :param optimizer: None, momentum and Adam, select one of the optimizer\n","\n","        :type weight_decay_lambda: float\n","        :param weight_decay_lambda: constant value of the weight decay, Normally is 0.9\n","\n","        :type momentum: float\n","        :param momentum: constant value of the momentum in SGD. Normally is 0.9\n","\n","        :type iter: int\n","        :param iter: epoch of the training process\n","\n","        :type rho1: float\n","        :param rho1: exponential decay rate for the first moment estimates. Normally is 0.9\n","\n","        :type rho2: float\n","        :param rho2: exponential decay rate for the second-moment estimates. Normally is 0.999\n","\n","        :type epsilon: float\n","        :param epsilon: constant value for numerical stability. Normally is 1e-8\n","        \"\"\"\n","        for layer in self.layers:\n","            # Normal mode with no optimizer\n","            if optimizer is None:\n","                # update the weight with the weight decay\n","                # According to the lecture 4: 𝜃 = (1−𝜂𝛼)𝜃 - 𝜂𝛻(𝜃)\n","                layer.W = (1 - lr * weight_decay_lambda) * layer.W - lr * layer.grad_W\n","                layer.b -= lr * layer.grad_b\n","\n","                # for batch normalization parameter with the weight decay\n","                layer.gamma = (1 - lr * weight_decay_lambda) * layer.gamma - lr * layer.grad_gamma\n","                layer.beta -= lr * layer.grad_beta\n","\n","            # Momentum with SGD\n","            elif optimizer.lower() == 'momentum':\n","                # update the weight in momentum with the weight decay\n","                # According to the lecture 3\n","                # v = momentum * v + learning_rate * gradient\n","                # w = w - v\n","                layer.grad_W_V = momentum * layer.grad_W_V + lr * layer.grad_W\n","                layer.grad_b_V = momentum * layer.grad_b_V + lr * layer.grad_b\n","                layer.W = (1 - lr * weight_decay_lambda) * layer.W - layer.grad_W_V\n","                layer.b = layer.b - layer.grad_b_V\n","\n","                # for batch normalization parameter with momentum and weight decay\n","                layer.grad_gamma_V = momentum * layer.grad_gamma_V + lr * layer.grad_gamma\n","                layer.grad_beta_V = momentum * layer.grad_beta_V + lr * layer.grad_beta\n","                layer.gamma = (1 - lr * weight_decay_lambda) * layer.gamma - layer.grad_gamma_V\n","                layer.beta = layer.beta - layer.grad_beta_V\n","\n","            # Adam\n","            elif optimizer.lower() == 'adam':\n","                # update the weight in Adam with the weight decay\n","                # According to the lecture 3\n","                # mt = beta1 * mt + (1-beta1) * gradient\n","                # vt = beta2 * vt + (1-beta2) * (gradient**2)\n","                layer.grad_W_mt = rho1 * layer.grad_W_mt + (1 - rho1) * layer.grad_W\n","                layer.grad_W_vt = rho2 * layer.grad_W_vt + (1 - rho2) * (layer.grad_W ** 2)\n","                layer.grad_b_mt = rho1 * layer.grad_b_mt + (1 - rho1) * layer.grad_b\n","                layer.grad_b_vt = rho2 * layer.grad_b_vt + (1 - rho2) * (layer.grad_b ** 2)\n","                # mt_vector = mt / (1-beta1**iter)\n","                # vt_vector = vt / (1-beta2**iter)\n","                w_mt_vector = layer.grad_W_mt / (1 - rho1 ** iter)\n","                w_vt_vector = layer.grad_W_vt / (1 - rho2 ** iter)\n","                b_mt_vector = layer.grad_b_mt / (1 - rho1 ** iter)\n","                b_vt_vector = layer.grad_b_vt / (1 - rho2 ** iter)\n","                # w = w - lr * mt_vector / (np.sqrt(vt_vector + epsilon))\n","                layer.W = (1 - lr * weight_decay_lambda) * layer.W - lr * w_mt_vector / (np.sqrt(w_vt_vector + epsilon))\n","                layer.b = layer.b - lr * b_mt_vector / (np.sqrt(b_vt_vector + epsilon))\n","\n","                \"\"\"--------------------Advanced Module-----Adam--------------\"\"\"\n","                # for batch normalization parameter with Adam and weight decay. Adavanced module\n","                layer.grad_gamma_mt = rho1 * layer.grad_gamma_mt + (1 - rho1) * layer.grad_gamma\n","                layer.grad_gamma_vt = rho2 * layer.grad_gamma_vt + (1 - rho2) * (layer.grad_gamma ** 2)\n","                layer.grad_beta_mt = rho1 * layer.grad_beta_mt + (1 - rho1) * layer.grad_beta\n","                layer.grad_beta_vt = rho2 * layer.grad_beta_vt + (1 - rho2) * (layer.grad_beta ** 2)\n","                gamma_mt_vector = layer.grad_gamma_mt / (1 - rho1 ** iter)\n","                gamma_vt_vector = layer.grad_gamma_vt / (1 - rho2 ** iter)\n","                beta_mt_vector = layer.grad_beta_mt / (1 - rho1 ** iter)\n","                beta_vt_vector = layer.grad_beta_vt / (1 - rho2 ** iter)\n","                layer.gamma = (1 - lr * weight_decay_lambda) * layer.gamma - lr * gamma_mt_vector / (\n","                    np.sqrt(gamma_vt_vector + epsilon))\n","                layer.beta = layer.beta - lr * beta_mt_vector / (np.sqrt(beta_vt_vector + epsilon))\n","\n","    def fit(self, X, y, learning_rate=0.1, epochs=100, mini_batch_size=128, optimizer=None, weight_decay_lambda=1,\n","            momentum=0.9, rho1=0.9,\n","            rho2=0.999, epsilon=1e-8, dropout_rate=0.5):\n","        \"\"\"\n","        Online Learning\n","        Define the training function\n","        It will return all losses within the whole training process.\n","\n","        :type X: numpy.array\n","        :param X: data of the training dataset\n","\n","        :type y: numpy.array\n","        :param y: label of the training dataset\n","\n","        :type learning_rate: float\n","        :param learning_rate: learning rate for the update\n","\n","        :type epochs: int\n","        :param epochs: epoch of the training process\n","\n","        :type mini_batch_size: int\n","        :param mini_batch_size: size of each batch\n","\n","        :type optimizer: str\n","        :param optimizer: None, momentum and Adam, select one of the optimizer\n","\n","        :type weight_decay_lambda: float\n","        :param weight_decay_lambda: constant value of the weight decay, Normally is 0.9\n","\n","        :type momentum: float\n","        :param momentum: constant value of the momentum in SGD. Normally is 0.9\n","\n","        :type rho1: float\n","        :param rho1: exponential decay rate for the first moment estimates. Normally is 0.9\n","\n","        :type rho2: float\n","        :param rho2: exponential decay rate for the second-moment estimates. Normally is 0.999\n","\n","        :type epsilon: float\n","        :param epsilon: constant value for numerical stability. Normally is 1e-8\n","\n","        :type dropout_rate: float\n","        :param dropout_rate: rate for the dropout\n","\n","        :return: loss, accuracy of the training set, accuracy of the validation set\n","        \"\"\"\n","        X = np.array(X)\n","        y = np.array(y)\n","        # Initialize the loss, accuracy of the training set, accuracy of the validation set for all epoch\n","        to_return = np.zeros(epochs)\n","        train_accuracy = np.zeros(epochs)\n","        val_accuracy = np.zeros(epochs)\n","\n","        for k in range(epochs):\n","            # Partition of the training dataset\n","            mini_batch_X, mini_batch_Y = self.mini_batch(X, y, mini_batch_size)\n","            loss = np.zeros(mini_batch_X.shape[0])\n","\n","            for it in range(mini_batch_X.shape[0]):\n","                # Forward pass\n","                y_hat = self.forward(mini_batch_X[it], dropout_rate)\n","                # Loss function and backward pass\n","                loss[it], delta = self.criterion_CE(mini_batch_Y[it], y_hat)\n","                self.backward(delta)\n","                # Update\n","                self.update(lr=learning_rate, optimizer=optimizer, weight_decay_lambda=weight_decay_lambda,\n","                            momentum=momentum, iter=epochs, rho1=rho1, rho2=rho2, epsilon=epsilon)\n","\n","            # Store the loss for one epoch\n","            to_return[k] = np.mean(loss)\n","\n","            # Predict the accuracy of the training and validation data for one epoch\n","            train_pre = nn.predict(X_train)\n","            train_acc = accuracy_score(y_train, train_pre)\n","            val_pre = nn.predict(X_val)\n","            val_acc = accuracy_score(y_val, val_pre)\n","\n","            # Store the accuracy for one epoch and print\n","            val_accuracy[k] = val_acc\n","            train_accuracy[k] = train_acc\n","            print(\n","                'epoch {0} loss: {1:.6f} train_accuracy: {2:.6f} validate_accuracy: {3:.6f}'.format(k,\n","                                                                                                    float(to_return[k]),\n","                                                                                                    train_acc, val_acc))\n","        return to_return, train_accuracy, val_accuracy\n","\n","    def mini_batch(self, X, Y, batch_size=64):\n","        \"\"\"\n","        Define the function for the mini batch training\n","        Divide the training dataset to the several batches\n","        Each batch has the size of the batch_size\n","        If the last batch does not have the enough data, then full it to the batch size.\n","\n","        :type X: numpy.array\n","        :param X: data of the training dataset\n","\n","        :type Y: numpy.array\n","        :param Y: label of the training dataset\n","\n","        :type batch_size: int\n","        :param batch_size: size of each batch\n","\n","        :return: mini batch of the features and label in the type of numpy array\n","        \"\"\"\n","        # Initialize the random seed\n","        np.random.seed(2023)\n","        mini_batch_X = []\n","        mini_batch_Y = []\n","        # Get the number of training example\n","        m = X.shape[0]\n","\n","        # Shuffle (X, Y), m is the num of instances of data set\n","        permutation = np.random.permutation(m)\n","        shuffled_X = X[permutation, :]\n","        shuffled_Y = Y[permutation, :].reshape((m, 1))\n","\n","        # Split the training data into batch_size=64\n","        # Round down\n","        num_batches = math.floor(m / batch_size)\n","        # Partition\n","        for i in range(0, num_batches):\n","            batch_X = shuffled_X[i * batch_size:(i + 1) * batch_size, :]\n","            batch_Y = shuffled_Y[i * batch_size:(i + 1) * batch_size, :]\n","            mini_batch_X.append(batch_X)\n","            mini_batch_Y.append(batch_Y)\n","\n","        # If there are remaining training examples, add them into the last batch\n","        if m % batch_size != 0:\n","            batch_X = shuffled_X[num_batches * batch_size:, :]\n","            batch_Y = shuffled_Y[num_batches * batch_size:, :]\n","            # Calculate the minus between the number of data in the last batch and the batch size\n","            # Then Calculate the number required to full of the batch\n","            add_X = shuffled_X[m - (num_batches * batch_size) - batch_size:, :]\n","            add_Y = shuffled_Y[m - (num_batches * batch_size) - batch_size:, :]\n","            # Combine the rest data and the additional data to an integral batch\n","            batch_X = np.vstack((batch_X, add_X))\n","            batch_Y = np.vstack((batch_Y, add_Y))\n","            mini_batch_X.append(batch_X)\n","            mini_batch_Y.append(batch_Y)\n","\n","        # Return the mini batch of the features and label\n","        return np.array(mini_batch_X), np.array(mini_batch_Y)\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Define the prediction function\n","        We can use predict function to predict the results of new data, by using the well-trained network.\n","\n","        :type x: numpy.array\n","        :param x: data of the dataset\n","\n","        :return: predict label of the input dataset\n","        \"\"\"\n","        x = np.array(x)\n","        pred = np.zeros(x.shape[0])\n","        # Put all data to the forward progress\n","        output = nn.forward(x[:, :], test_flg=True)\n","        # Softmax progress\n","        exps = np.exp(output - np.max(output, axis=1, keepdims=True))\n","        output = exps / np.sum(exps, axis=1, keepdims=True)\n","        # Select the max one as the label\n","        pred = np.argmax(output, axis=1)\n","        return pred"]},{"cell_type":"markdown","metadata":{"id":"6bSpaotl3bAV"},"source":["## Learning"]},{"cell_type":"markdown","source":["To change the hyper parameters for multiple training. You can change them.\\\n","$$activation: \"relu\", \"gelu\"\\\\\n","dropout: True, False\\\\\n","batch\\_norm: True, False\\\\\n","optimizer: None, \"momentum\", \"adam\"$$"],"metadata":{"id":"6_bWa97Psehx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Er_w2_643b-o"},"outputs":[],"source":["0# Try different MLP models, dropout, batch normalization\n","# You can set different nodes of hidden layers, activations, and options for dropout, batch normalization\n","# But notice that the number of the hidden layers must equal to the activation layers\n","# activation layer: \"relu\", \"gelu\"\n","# dropout and batch_norm: True, False\n","nn = MLP([128, 1280, 640, 320, 120, 32, 10], [None, 'relu', 'relu', 'relu', 'relu', 'relu', 'relu'], dropout=True,\n","         batch_norm=True)\n","\n","# Try different hyperparameter\n","# You can set different value of hyperparameter and optimizer.\n","# optimizer: \"momentum\", \"adam\", None\n","# hyperparameter: learning_rate, epochs, mini_batch_size, weight_decay_lambda, dropout_rate\n","loss, train_acc, validata_acc = nn.fit(X_train, y_train, learning_rate=0.001, epochs=200, mini_batch_size=64,\n","                                       optimizer='momentum', weight_decay_lambda=0.0001, momentum=0.9, rho1=0.9,\n","                                       rho2=0.999, dropout_rate=0.4)"]},{"cell_type":"markdown","metadata":{"id":"GSKb-o8t346v"},"source":["### Plot loss in epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUesjZVH5v00"},"outputs":[],"source":["# Plot the loss figure\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(loss)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nCzvHdJ4Z55X"},"source":["### Plot accuracy in epoch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vENUw7DHZ90J"},"outputs":[],"source":["# Plot the figure\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.plot(train_acc, label='training_accuracy')\n","plt.plot(validata_acc, label='validation_accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"myxR6PDq53sd"},"source":["## Testing"]},{"cell_type":"markdown","metadata":{"id":"Ote_7pdT6MYe"},"source":["### Print the results of different evaluation metrics\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zRuNnnh577g"},"outputs":[],"source":["# Calculate the accuracy of the test dataset\n","_pre = nn.predict(X_test)\n","acc = accuracy_score(y_test, _pre)\n","f1 = f1_score(y_test, _pre, average='macro')\n","recall = recall_score(y_test, _pre, average='macro')\n","precision = precision_score(y_test, _pre, average='macro')\n","print('Test-accuracy: {0:.4f}, F1_score: {1:.4f}, Recall: {2:.4f}, Precision: {3:.4f}'.format(acc, f1, recall,\n","                                                                                              precision))"]},{"cell_type":"markdown","metadata":{"id":"PtxH1dyj6ldI"},"source":["## End time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ok5N1YCv6gU8"},"outputs":[],"source":["# Calculate the code run time\n","end = time.time()\n","print(\"Code running Time: {:.3f} min\".format((end - start)/60))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}