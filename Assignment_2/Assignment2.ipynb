{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# COMP5329 Assignment2"],"metadata":{"id":"Yfehh0vctTl9"}},{"cell_type":"markdown","source":["There are eight sections in this ipynb file, run each code block sequnetly.\n","\n","Each markdown note in the head of each section describe general idea of what this part code is doing."],"metadata":{"id":"rtfFCSAGS4yn"}},{"cell_type":"markdown","source":["## 1. Connect to the Google Drive and Download the Data"],"metadata":{"id":"cK1qU9rhtc_6"}},{"cell_type":"markdown","source":["This section is used to download the dataset in the shared folder to your own google drive in order to load the dataset later."],"metadata":{"id":"MwjgI0zBRYz3"}},{"cell_type":"code","source":["from pydrive.auth import GoogleAuth\n","from google.colab import drive\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"metadata":{"id":"bS9xNW-ZPWJZ","executionInfo":{"status":"ok","timestamp":1684409305806,"user_tz":-600,"elapsed":759,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","train_data = '1TsZTJ6njohRMcl9NbmEWbeEukegDG16a'\n","test_data = '1ilw0qHPWzL3_JgSFgXBbSiP17J-csAin'\n","img_data = '1iGqrYPewPyrPSG8zn48N7Jawg9DdrSUV'\n","\n","download_train_data = drive.CreateFile({'id': train_data})\n","download_test_data = drive.CreateFile({'id': test_data})\n","download_img_data = drive.CreateFile({'id': img_data})\n","\n","# Download the file to a local disc\n","download_train_data.GetContentFile('train.csv')\n","download_test_data.GetContentFile('test.csv')\n","download_img_data.GetContentFile('data.zip')"],"metadata":{"id":"52DdEGB2PYnz","executionInfo":{"status":"ok","timestamp":1684409328888,"user_tz":-600,"elapsed":23095,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["! unzip -uq data.zip"],"metadata":{"id":"SQtqBFixaiuz","executionInfo":{"status":"ok","timestamp":1684409340432,"user_tz":-600,"elapsed":11590,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## 2. Library and Parameters"],"metadata":{"id":"XgHch_0z6gls"}},{"cell_type":"markdown","source":["This section includs importing modules, downloading nlp related tool kits and set up cuda environment, defining file directory.\n","\n"],"metadata":{"id":"tjVP_LxlZQhu"}},{"cell_type":"markdown","source":["### 2.1 Import the Packages"],"metadata":{"id":"2GPxg-mQt5AW"}},{"cell_type":"code","source":["import os\n","import re\n","import time\n","import torch\n","import gensim.downloader\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","import skimage.io as io\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from io import StringIO\n","from torchvision import transforms, models\n","from collections import Counter\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")  # Ignore the warning of the sklearn version"],"metadata":{"id":"0yzYGqVn5ZRE","executionInfo":{"status":"ok","timestamp":1684409346019,"user_tz":-600,"elapsed":5595,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### 2.2 Download the nltk"],"metadata":{"id":"YQ9yl_5Ttncn"}},{"cell_type":"markdown","source":["Use the nltk as the word processing tool."],"metadata":{"id":"-K6-hYBURzgI"}},{"cell_type":"code","source":["import nltk\n","\n","nltk.download('omw-1.4')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer"],"metadata":{"id":"2cfvnCnkaNYl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684409347208,"user_tz":-600,"elapsed":1206,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}},"outputId":"59f8c590-d8cb-4153-c75f-fba3389d2e10"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}]},{"cell_type":"markdown","source":["### 2.3 Set Global Parameters and Download the Word Embedding Model"],"metadata":{"id":"OvGA6IpZttXK"}},{"cell_type":"markdown","source":["Set the datafile and the hyper parameters. Some parameters are flobal parameter for model and training use.\n","\n","Then using the gensim.downloader as the tool to download the $glove$ pre-trained word embedding model."],"metadata":{"id":"QpFc-4C3SDyV"}},{"cell_type":"code","source":["# File path and the Global parameters\n","TRAIN_CSV = \"train.csv\"\n","TEST_CSV = \"test.csv\"\n","IMAGE_DIR = \"data\"\n","TRAIN_VAL_PROP = 0.8\n","BATCH_SIZE = 32\n","SEED = 2023\n","LR = 0.001\n","MAX_EPOCH = 15\n","THRESHOLD = 0.3\n","NUM_CLASS = 18\n","EMBEDDING_MODEL = gensim.downloader.load('glove-wiki-gigaword-50')  # Download the glove-wiki-gigaword-50 model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Set the training device of CUDA"],"metadata":{"id":"3tz4UBXNaP1F","executionInfo":{"status":"ok","timestamp":1684409379368,"user_tz":-600,"elapsed":32169,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1bee66bb-05dd-4097-ba03-2f693a11b504"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[=================================================-] 98.6% 65.1/66.0MB downloaded\n"]}]},{"cell_type":"markdown","source":["## 3. Correct the Error Data in the train.csv"],"metadata":{"id":"Hror4guhuBhZ"}},{"cell_type":"markdown","source":["There are several error datas in the train csv, correct them."],"metadata":{"id":"GI4MPUDOYT-A"}},{"cell_type":"code","source":["# Correct the error data item in the train data file\n","with open(TRAIN_CSV) as fp:\n","    data = fp.readlines()\n","\n","data[4790] = data[4790].replace(\"/\", \"\")\n","data[14716] = data[14716].replace(\"/\", \"\")\n","data[14961] = data[14961].replace(\"/\", \"\")\n","data[29895] = data[29895].replace(\"/\", \"\")\n","\n","# Write back\n","with open(TRAIN_CSV, 'w') as file:\n","    file.writelines(data)"],"metadata":{"id":"nSBrW4L35a8Y","executionInfo":{"status":"ok","timestamp":1684409379370,"user_tz":-600,"elapsed":44,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## 4. Caption Embedding"],"metadata":{"id":"_lx2YNhK5tH0"}},{"cell_type":"markdown","source":["This section includes the NLP related data preprocessing functions.\n","\n","Extract the words in captions and create the embedding model.\n","\n","For captions, this processing includes lower cases, remove punctuations, tokenization, lemmatization, keep 'NN' type words, keeping words that appears more than once, finally, pad each caption to equal length. After this, gensim.downloader.load(\"glove-wiki-gigaword-50\") will be used to find each word with its embeddings."],"metadata":{"id":"AnHNOxmCcSZp"}},{"cell_type":"markdown","source":["### 4.1 Extract the Words in Caption"],"metadata":{"id":"kVe3ZuiNuZG6"}},{"cell_type":"markdown","source":["Processes captions from train and test files and extracts main feature words."],"metadata":{"id":"c2tuU0i7cu5m"}},{"cell_type":"code","source":["def caption_extract(caption):\n","    \"\"\"\n","    This function takes a caption as input and extracts the main feature words from it.\n","    It also performs preprocessing by removing non-meaningful letters and numbers from the caption.\n","    Delete the words in error.\n","\n","    :param caption: input caption, which can be either a string or a list of strings\n","    :return: a list of extracted main feature words from each caption\n","    \"\"\"\n","    caption_in_word = []\n","    # Set of stopwords\n","    stop_words = set(stopwords.words('english'))\n","\n","    # Lemmatizer object\n","    lemmatizer = WordNetLemmatizer()\n","\n","    for sentence in caption:\n","        # Convert the caption to lowercase and remove non-alphabetic characters and replace with space\n","        sentence = sentence.lower()\n","        sentence = re.sub(r'[^A-Za-z]+', ' ', sentence)\n","        # Split the caption into a list of words\n","        words = sentence.split()\n","\n","        # Remove stopwords and perform lemmatization\n","        filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n","\n","        # Set of word errors\n","        word_error = {'baeball', 'basball', 'blackandwhite', 'bluewhite', 'checkerd', 'firehydrant',\n","                      'firsbee', 'fourwheeler', 'frisbe', 'frizbee', 'kiteboards', 'krispee',\n","                      'midswing', 'parasailers', 'skiboard', 'skii', 'skiies', 'surfboarder',\n","                      'surfboarding', 'tball', 'umbrells', 'windsurfs', 'deckered', 'rared',\n","                      'snowcovered'}\n","\n","        # Filter out words with errors\n","        correct_word = [word for word in filtered_words if not word in word_error]\n","        caption_in_word.append(correct_word)\n","\n","    return caption_in_word\n","\n","\n","def process_captions(train_file_path, test_file_path):\n","    \"\"\"\n","    Processes captions from train and test files, performs preprocessing, and extracts main feature words.\n","\n","    :param train_file_path: file path of the train captions file\n","    :param test_file_path:  path of the test captions file\n","    :return: processed captions and the maximum length among all captions\n","    \"\"\"\n","    # Processing train captions\n","    with open(train_file_path) as train_file:\n","        # Preprocessing step to handle quotes inside the captions\n","        train_lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in train_file]\n","        train_caption = pd.read_csv(StringIO(''.join(train_lines)), escapechar=\"/\")\n","        train_caption = train_caption.drop(columns='Caption').join(train_caption['Caption'].str.replace('\\\"', ''))\n","\n","    # Processing test captions\n","    with open(test_file_path) as test_file:\n","        # Preprocessing step to handle quotes inside the captions\n","        test_lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in test_file]\n","        test_caption = pd.read_csv(StringIO(''.join(test_lines)), escapechar=\"/\")\n","        test_caption = test_caption.drop(columns='Caption').join(test_caption['Caption'].str.replace('\\\"', ''))\n","\n","    # Extracting main feature words from concatenated train and test captions\n","    caption = caption_extract(pd.concat([train_caption['Caption'], test_caption['Caption']], axis=0, ignore_index=True))\n","\n","    # Finding the maximum length among all captions\n","    max_len = max(len(s) for s in caption)\n","\n","    return caption, max_len\n","\n","\n","whole_caption, max_caption_len = process_captions(TRAIN_CSV, TEST_CSV)\n","# vector size of each word\n","embedding_dim = EMBEDDING_MODEL.vector_size"],"metadata":{"id":"I3nxddLB5WCP","executionInfo":{"status":"ok","timestamp":1684409381798,"user_tz":-600,"elapsed":2469,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### 4.2 Creates a Vocabulary Dictionary and Word Embedding Model"],"metadata":{"id":"wMqCipj12Lxg"}},{"cell_type":"markdown","source":["Creates a word model basing on the pre-trained word embedding model."],"metadata":{"id":"dEJ4bS3ic5_q"}},{"cell_type":"code","source":["def create_word_model(caption):\n","    \"\"\"\n","    Creates a word model by extracting words from captions basing on the pre-trained word embedding model.\n","    Building a vocabulary set and creating word embeddings.\n","\n","    :param caption: list of captions, where each caption is a list of words\n","    :return: vocabulary dictionary and word embedding table\n","    \"\"\"\n","    # Extract words from the caption\n","    word_list = [word for sentence in caption for word in sentence]\n","\n","    # Count word occurrences in the caption\n","    word_counter = Counter(word_list)\n","\n","    # Select the words with more than 1 appearance for the vocabulary set\n","    vocab_set = {word for word, count in word_counter.items() if count > 4}\n","\n","    # Add the PAD and UNKNOWN to the vocabulary set\n","    vocab_set.update(['[PAD]', '[UNKNOWN]'])\n","\n","    # Sort the vocabulary set\n","    vocab_list = sorted(vocab_set)\n","\n","    # Create the word dictionary and embedding table based on the pre-trained word embedding model\n","    vocab_dictionary = {}\n","    embedding_table = []\n","    for i, word in enumerate(vocab_list):\n","        vocab_dictionary[word] = i\n","        # If the word is in the pre-trained word embedding model, add word embedding to the table\n","        if word in EMBEDDING_MODEL:\n","            embedding_table.append(EMBEDDING_MODEL[word])\n","        else:\n","            embedding_table.append([0] * embedding_dim)\n","    embedding_table = np.array(embedding_table)\n","\n","    return vocab_dictionary, embedding_table\n","\n","\n","vocab_dict, emb_table = create_word_model(whole_caption)"],"metadata":{"id":"1v2Cb2Fu2VAy","executionInfo":{"status":"ok","timestamp":1684409381799,"user_tz":-600,"elapsed":13,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### 4.3 Tokenizer"],"metadata":{"id":"aqceWjzUunOZ"}},{"cell_type":"code","source":["def tokenizer(caption):\n","    \"\"\"\n","    Tokenizes captions by converting words into corresponding indices based on the vocabulary dictionary.\n","\n","    :param caption: list of captions, where each caption is a list of words\n","    :return: list of tokenized captions, where each caption is a list of word indices\n","    \"\"\"\n","    tokenize = []\n","    # Extract main feature words from captions\n","    caption = caption_extract(caption)\n","\n","    for item in caption:\n","        # Convert each word to its corresponding index in the vocabulary dictionary\n","        # If a word is not present in the dictionary, use the index for the [UNKNOWN] token\n","        temp = [vocab_dict[word] if word in vocab_dict else vocab_dict['[UNKNOWN]'] for word in item]\n","        if len(temp) < max_caption_len:\n","            # If the caption is shorter than the maximum length\n","            # Pad the caption with [PAD] tokens to make it of maximum length\n","            temp += [vocab_dict['[PAD]']] * (max_caption_len - len(temp))\n","        else:\n","            # Truncate the caption if it exceeds the maximum length\n","            temp = temp[:max_caption_len]\n","        tokenize.append(temp)\n","\n","    return tokenize"],"metadata":{"id":"u0hBdqDM5hYj","executionInfo":{"status":"ok","timestamp":1684409381800,"user_tz":-600,"elapsed":13,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## 5. Dataloader"],"metadata":{"id":"9eUoDDB058Fh"}},{"cell_type":"markdown","source":["For labels, transfer the label to one-hot code.\n","\n","For images, torchvision.transform will be used to generate unify input. This procedure includes resizing, into tensor, and normalization."],"metadata":{"id":"SspU1xKzfAGX"}},{"cell_type":"markdown","source":["### 5.1 Custom Dataset"],"metadata":{"id":"dTt2Yzt8usMh"}},{"cell_type":"markdown","source":["Define the dataset."],"metadata":{"id":"phM7jePZfS0c"}},{"cell_type":"code","source":["class DataLoad(torch.utils.data.Dataset):\n","    \"\"\"\n","    Custom dataset class for loading data, including images and captions.\n","\n","    Args:\n","        data_file (str): path to the data file\n","        image (str): directory containing the images\n","        transform (object): optional image transformation to be applied\n","        text_csv (bool): whether the data file includes caption text in a separate CSV file\n","    \"\"\"\n","\n","    def __init__(self, data_file, image, transform=None, text_csv=None):\n","        \"\"\"\n","        Initialize the dataset.\n","\n","        :param data_file: path to the data file\n","        :param image: directory containing the images\n","        :param transform: optional image transformation to be applied\n","        :param text_csv: whether the data file includes caption text in a separate CSV file\n","        \"\"\"\n","        self.image_dir = image\n","        self.text_csv = text_csv\n","        self.transform = transform\n","        self.classes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19]  # List of classes\n","\n","        with open(data_file) as data_file:\n","            # Remove the 'Caption' column and join the 'Caption' values after removing quotes\n","            lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in data_file]\n","            dataframe = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n","            self.dataframe = dataframe.drop(columns='Caption').join(dataframe['Caption'].str.replace('\\\"', ''))\n","\n","    def __len__(self):\n","        \"\"\"\n","        Get the total number of samples in the dataset.\n","\n","        :return: total number of samples\n","        \"\"\"\n","        return self.dataframe.shape[0]\n","\n","    def __getitem__(self, item):\n","        \"\"\"\n","        Get a specific sample from the dataset.\n","\n","        :param item: index of the sample\n","        :return: dictionary containing the image, label, image ID, and caption\n","        \"\"\"\n","        if torch.is_tensor(item):\n","            item.to_list()\n","\n","        # Combine the image path with the image file name\n","        img_path = os.path.join(self.image_dir, self.dataframe.iloc[item, 0])\n","        img = io.imread(img_path)\n","        img_id = self.dataframe.iloc[item, 0]\n","\n","        if not self.text_csv:\n","            img_caption = self.dataframe.iloc[item, 2]\n","            # Get the image labels and split them\n","            img_label = self.dataframe.iloc[item, 1].split(' ')\n","            # Convert labels to integers\n","            img_label = [int(x) for x in img_label]\n","\n","            for i in range(len(img_label)):\n","                # One-hot encode and sum the image labels\n","                img_label[i] = [1 if cls == img_label[i] else 0 for cls in self.classes]\n","            img_label = sum(torch.tensor(img_label, dtype=torch.float))\n","\n","            if self.transform:\n","                img = self.transform(img)\n","\n","            sample = {'img': img, 'label': img_label, 'id': img_id, 'caption': img_caption}\n","        else:\n","            img_caption = self.dataframe.iloc[item, 1]\n","            if self.transform:\n","                img = self.transform(img)\n","            sample = {'img': img, 'id': img_id, 'caption': img_caption}\n","\n","        return sample"],"metadata":{"id":"N-UJCrPhaRVb","executionInfo":{"status":"ok","timestamp":1684409381803,"user_tz":-600,"elapsed":15,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### 5.2 Image Transforms and torch Dataloader"],"metadata":{"id":"INsWLNU7u0wM"}},{"cell_type":"code","source":["# Image Pre-Processing\n","transforms = {\n","    'train': transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Resize((256, 256)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(),\n","        transforms.RandomAffine(degrees=20, translate=(0.2, 0.2), scale=(0.5, 1.5),\n","                                shear=None,\n","                                fill=tuple(np.array(np.array([0.485, 0.456, 0.406]) * 255).astype(int).tolist())),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Resize((256, 256)),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","}\n","\n","dataset = DataLoad(data_file=TRAIN_CSV, image=IMAGE_DIR, transform=transforms['train'], text_csv=False)\n","test_dataset = DataLoad(data_file=TEST_CSV, image=IMAGE_DIR, transform=transforms['val'], text_csv=True)\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(TRAIN_VAL_PROP * len(dataset)),\n","                                                                     len(dataset) - (\n","                                                                         int(TRAIN_VAL_PROP * len(dataset)))])\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n","val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=8, shuffle=False, num_workers=0)"],"metadata":{"id":"RF0EjS1N7PKv","executionInfo":{"status":"ok","timestamp":1684409381805,"user_tz":-600,"elapsed":16,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## 6. The Model"],"metadata":{"id":"wlqr8IqJu_Dn"}},{"cell_type":"markdown","source":["Build network, which conbines pretrained efficient_b0 model for extracting image features and LSTM for obtaining language information."],"metadata":{"id":"BqlApUqqffnL"}},{"cell_type":"code","source":["class Model(nn.Module):\n","    \"\"\"\n","    The model for the image and natural language classification.\n","    The image classifier uses the pre-trained EFFICIENTNET_B1 with the pre-trained weights.\n","    The natural language classifier uses the LSTM as the model process.\n","\n","    Args:\n","        emb_table (np.ndarray): pretrained word embedding table\n","\n","    Attributes:\n","        efficient_net (nn.Module): EfficientNet-B1 model as the feature extractor\n","        emb (nn.Embedding): embedding layer, loads the pre-trained embedding table\n","        lstm (nn.LSTM): LSTM layer for processing text sequences\n","        linear (nn.Linear): linear layer for linear transformation of LSTM output\n","        classifier (nn.Linear): classifier layer for the final classification task\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        # Use EfficientNet-B1 model as the feature extractor\n","        weights = models.EfficientNet_B1_Weights.DEFAULT\n","        self.efficient_net = models.efficientnet_b1(weights=weights)\n","        # Modify the classifier layer to have an output dimension of 64\n","        self.efficient_net.classifier = nn.Sequential(\n","            nn.Dropout(p=0.5),\n","            nn.Linear(in_features=self.efficient_net.classifier[1].in_features, out_features=64)\n","        )\n","        # Freeze the parameters of EfficientNet and only train the classifier layer\n","        for param in self.efficient_net.parameters():\n","            param.requires_grad = False\n","        for param in self.efficient_net.classifier.parameters():\n","            param.requires_grad = True\n","\n","        # Create an embedding layer and load the pre-trained embedding table\n","        self.emb = nn.Embedding(num_embeddings=emb_table.shape[0], embedding_dim=emb_table.shape[1])\n","        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n","        self.emb.weight.requires_grad = False\n","        # Create an LSTM layer\n","        self.lstm = nn.LSTM(input_size=emb_table.shape[1], hidden_size=emb_table.shape[1], num_layers=2,\n","                            batch_first=True, dropout=0.2)\n","        # Create a linear layer for linear transformation of LSTM output\n","        self.linear = nn.Linear(in_features=emb_table.shape[1] * 2, out_features=64)\n","\n","        # Create a classifier layer with an output dimension of num classes\n","        self.classifier = nn.Linear(in_features=128, out_features=NUM_CLASS)\n","\n","    def forward(self, image, caption):\n","        \"\"\"\n","        Forward propagation of the model.\n","\n","        :param image: input image tensor\n","        :param caption: input text sequence tensor\n","        :return: output tensor of the model\n","        \"\"\"\n","        net_out = self.efficient_net(image)\n","\n","        x = self.emb(caption)\n","        # Pass the embedded caption through the LSTM layer and get the final hidden states (h) and cell states (c)\n","        _, (h, c) = self.lstm(x)\n","        # Concatenate the final hidden states from both directions of the LSTM\n","        x = torch.cat((h[0, :, :], h[1, :, :]), 1)\n","        lstm_out = self.linear(x)\n","\n","        # Concatenate image features and text features, and pass through the classifier layer for classification\n","        output = self.classifier(torch.cat((net_out, lstm_out), 1))\n","\n","        return output\n","\n","\n","net_model = Model().to(device)"],"metadata":{"id":"iW6CIwl25kgL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.1 Calculate the Model Size"],"metadata":{"id":"myOCgtZrvEZ6"}},{"cell_type":"markdown","source":["Calculate the model size within 100 MB limited."],"metadata":{"id":"2ErW6OQyg28O"}},{"cell_type":"code","source":["def model_size(model):\n","    \"\"\"\n","    Calculate the size of the model with its word embeddings.\n","\n","    :param model: initialized model\n","    :return: None\n","    \"\"\"\n","    cnn_size = 0\n","    # Iterate over the named parameters of the model\n","    for name, param in model.named_parameters():\n","        # Calculate the size of the parameter in megabytes\n","        param_size = np.prod(list(param.shape)) * 4 / 1e6\n","        cnn_size += param_size\n","    \n","    # Calculate the embedding model size\n","    embedding_model_path = gensim.downloader.load('glove-wiki-gigaword-50', return_path=True)\n","    embedding_size = os.stat(embedding_model_path).st_size / (1024 * 1024)\n","\n","    print(\"Model size: {:4f} MB; Word Embedding Size: {:4f} MB\".format(cnn_size, embedding_size))\n","    print(\"Total size: {:4f} MB\".format((cnn_size + embedding_size)))\n","    if (cnn_size + embedding_size) > 100:\n","        raise ValueError(\"Model too large!\")\n","\n","\n","model_size(net_model)"],"metadata":{"id":"ObYA8T9OZdxy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Training Process"],"metadata":{"id":"U503J1O86Rte"}},{"cell_type":"markdown","source":["The training procedure will be:\n","\n","firstly call $train()$ and $evaluate()$, which includes the all iters and epochs training and evaluating. Then the $FocalLoss()$ could be used as the loss function.\n","\n","before training, set the optimizer and loss function.\n","\n","\n","other functions include $get_acc()$ which will return current precision and recall."],"metadata":{"id":"s2Aar_Zog-PT"}},{"cell_type":"markdown","source":["### 7.1 Calculate the Score\n","\n"],"metadata":{"id":"FRZ2yF1cvHIW"}},{"cell_type":"code","source":["def get_acc(output, label):\n","    \"\"\"\n","    Calculate precision, recall, and F1 score for model predictions.\n","\n","    :param output: predicted output from the model\n","    :param label: ground truth labels\n","    :return: micro precision score, recall score and f1 score\n","    \"\"\"\n","    # Convert the predicted output and labels to numpy arrays\n","    output = output.cpu().detach().numpy()\n","    label = label.cpu().numpy()\n","    predicted_labels = []\n","    for preds in output:\n","        # If there is no sigmoid output larger than threshold, set the second element as the predict label\n","        if sum(preds > THRESHOLD) == 0:\n","            temp = np.zeros(18)\n","            temp[np.argmax(preds[1])] = 1\n","            predicted_labels.append(temp)\n","        # If there is at least one sigmoid output larger than threshold\n","        else:\n","            predicted_labels.append(np.array(preds > THRESHOLD, dtype=float))\n","    pred = np.array(predicted_labels, dtype=float)\n","\n","    precision = precision_score(y_true=label, y_pred=pred, average='micro')\n","    recall = recall_score(y_true=label, y_pred=pred, average='micro')\n","    f1 = f1_score(y_true=label, y_pred=pred, average='micro')\n","\n","    return 100 * precision, 100 * recall, 100 * f1"],"metadata":{"id":"R6kd9b1569u4","executionInfo":{"status":"ok","timestamp":1684409383325,"user_tz":-600,"elapsed":11,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### 7.2 Training and Evaluating Function"],"metadata":{"id":"F0mWIz0PvLe4"}},{"cell_type":"code","source":["def train(model, dataloader, loss_fn, opt):\n","    \"\"\"\n","    Train the model on the training dataset.\n","\n","    :param model: model to be trained\n","    :param dataloader: DataLoader providing the training dataset\n","    :param loss_fn: loss function used for optimization\n","    :param opt: optimizer for updating the model's parameters\n","    :return: training loss, precision, recall, F1-score, and the number of processed batches\n","    \"\"\"\n","    train_loss = 0\n","    train_precision = 0\n","    train_recall = 0\n","    train_f1 = 0\n","    batch_size = 0\n","    batch_num = 0\n","    # Set the model to train mode\n","    model.train()\n","\n","    # batch item: img, label, id, caption\n","    for batch in tqdm(dataloader):\n","        # Caption: input_ids, token_type_ids, attention_mask\n","        img, label, caption = batch[\"img\"].to(device), batch[\"label\"].to(device), torch.from_numpy(\n","            np.array(tokenizer(batch[\"caption\"]))).to(device)\n","        # Zero the gradients\n","        opt.zero_grad()\n","        output = model(img, caption)\n","        loss = loss_fn(output, label)\n","        # Backward pass and optimization step\n","        loss.backward()\n","        opt.step()\n","\n","        precision, recall, f1 = get_acc(nn.Sigmoid()(output), label)\n","        batch_num += 1\n","        batch_size += 1\n","        # Accumulate the batch loss scaled by the batch size\n","        train_loss += loss.item() * img.size(0)\n","        train_precision += precision\n","        train_recall += recall\n","        train_f1 += f1\n","        print('\\rBatch[{}/{}] - loss: {:.6f}  precision: {:.4f}%  recall: {:.4f}%  F1_score: {:.4f}%'.format(batch_size,\n","                                                                                                             len(dataloader),\n","                                                                                                             loss.item() * img.size(\n","                                                                                                                 0),\n","                                                                                                             precision,\n","                                                                                                             recall,\n","                                                                                                             f1))\n","\n","    return train_loss, train_precision, train_recall, train_f1, batch_num\n","\n","\n","def evaluate(model, dataloader, loss_fn):\n","    \"\"\"\n","    Evaluate the performance of the model on the validation set.\n","\n","    :param model: model to evaluate\n","    :param dataloader: data loader for the validation set\n","    :param loss_fn: loss function used for evaluation\n","    :return: total validation loss, precision, recall, F1 score, and the number of batches\n","    \"\"\"\n","    val_loss = 0\n","    val_precision = 0\n","    val_recall = 0\n","    val_f1 = 0\n","    batch_num = 0\n","    # Set the model to evaluation mode\n","    model.eval()\n","\n","    # Disable gradient computation\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader):\n","            img, label, caption = batch[\"img\"].to(device), batch[\"label\"].to(device), torch.from_numpy(\n","                np.array(tokenizer(batch[\"caption\"]))).to(device)\n","            output = model(img, caption)\n","            loss = loss_fn(output, label)\n","\n","            precision, recall, f1 = get_acc(nn.Sigmoid()(output), label)\n","            batch_num += 1\n","            val_loss += loss.item() * img.size(0)\n","            val_precision += precision\n","            val_recall += recall\n","            val_f1 += f1\n","\n","    return val_loss, val_precision, val_recall, val_f1, batch_num\n","\n","\n","def fit(model, train_dataloader, val_dataloader, epochs, loss_fn, opt, shed):\n","    \"\"\"\n","    Train and evaluate model.\n","\n","    :param model: model to train\n","    :param train_dataloader: dataloader for the training data\n","    :param val_dataloader: dataloader for the validation data\n","    :param epochs: number of epochs to train\n","    :param loss_fn: loss function\n","    :param opt: optimizer\n","    :param shed: scheduler for adjusting learning rate\n","    :return: training and validation metrics\n","    \"\"\"\n","    # Record the start time\n","    start_time = time.time()\n","\n","    tr_loss_all = np.zeros(epochs)\n","    tr_precision_all = np.zeros(epochs)\n","    tr_recall_all = np.zeros(epochs)\n","    tr_f1_all = np.zeros(epochs)\n","    vl_loss_all = np.zeros(epochs)\n","    vl_precision_all = np.zeros(epochs)\n","    vl_recall_all = np.zeros(epochs)\n","    vl_f1_all = np.zeros(epochs)\n","\n","    best_f1 = 0\n","\n","    for epoch in range(epochs):\n","        train_loss, train_precision, train_recall, train_f1, train_batch_num = train(model, train_dataloader, loss_fn,\n","                                                                                     opt)\n","        val_loss, val_precision, val_recall, val_f1, val_batch_num = evaluate(model, val_dataloader, loss_fn)\n","        print('Epoch [{}/{}]'.format(epoch, epochs))\n","        print('Train - loss: {:.6f}  precision: {:.4f}%  recall: {:.4f}%  F1_score: {:.4f}%'.format(\n","            train_loss / len(train_dataset), train_precision / train_batch_num, train_recall / train_batch_num,\n","            train_f1 / train_batch_num))\n","        print('Val - loss: {:.6f}  precision: {:.4f}%  recall: {:.4f}%  F1_score: {:.4f}%'.format(\n","            val_loss / len(val_dataset), val_precision / val_batch_num, val_recall / val_batch_num,\n","            val_f1 / val_batch_num))\n","        print('-' * 19)\n","\n","        # Store metrics for the current epoch\n","        tr_loss_all[epoch] = train_loss / len(train_dataset)\n","        tr_precision_all[epoch] = train_precision / train_batch_num\n","        tr_recall_all[epoch] = train_recall / train_batch_num\n","        tr_f1_all[epoch] = train_f1 / train_batch_num\n","        vl_loss_all[epoch] = val_loss / len(val_dataset)\n","        vl_precision_all[epoch] = val_precision / val_batch_num\n","        vl_recall_all[epoch] = val_recall / val_batch_num\n","        vl_f1_all[epoch] = val_f1 / val_batch_num\n","\n","        # Adjust learning rate using the scheduler\n","        shed.step(train_loss / len(train_dataset))\n","\n","        # Save the model with the best F1 score so far\n","        if val_f1 / val_batch_num > best_f1:\n","            best_f1 = val_f1 / val_batch_num\n","            print('Saving best pre_model, f1: {:.4f}%\\n'.format(best_f1))\n","            torch.save(model.state_dict(), 'best_steps.pth')\n","\n","    # Calculate the duration of training\n","    duration = time.time() - start_time\n","    print('Training complete in {:.0f}m {:.0f}s'.format(duration // 60, duration % 60))\n","\n","    return tr_loss_all, tr_precision_all, tr_recall_all, tr_f1_all, vl_loss_all, vl_precision_all, vl_recall_all, vl_f1_all"],"metadata":{"id":"QNMXXTI-aVFh","executionInfo":{"status":"ok","timestamp":1684409383326,"user_tz":-600,"elapsed":12,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### 7.3 Focal Loss"],"metadata":{"id":"yZs7a5vvHusT"}},{"cell_type":"markdown","source":["Focal loss can be used to substitute the $BCEWithLogitsLoss()$"],"metadata":{"id":"vuMni-R9huqs"}},{"cell_type":"code","source":["class FocalLoss(nn.Module):\n","    \"\"\"\n","    Focal loss implementation for binary classification tasks.\n","\n","    Args:\n","        gamma (float): parameter controls the shape of the focal loss function, default is 2\n","        alpha (float): balancing factor used to adjust the weight between positive and negative samples, default is 0.25\n","    \"\"\"\n","\n","    def __init__(self, gamma=2, alpha=0.25):\n","        \"\"\"\n","        Initialize the FocalLoss module.\n","\n","        :param gamma: parameter controls the shape of the focal loss function, default is 2\n","        :param alpha: balancing factor used to adjust the weight between positive and negative samples, default is 0.25\n","        \"\"\"\n","        super().__init__()\n","        # Wraps focal loss around existing loss_fcn()\n","        self.loss_fcn = nn.BCEWithLogitsLoss()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        self.reduction = 'sum'\n","        # required to apply FL to each element\n","        self.loss_fcn.reduction = 'none'\n","\n","    def forward(self, pred, true):\n","        \"\"\"\n","        Compute the focal loss.\n","\n","        :param pred: predicted logits\n","        :param true: true labels\n","        :return: focal loss\n","        \"\"\"\n","        # Calculate the loss\n","        loss = self.loss_fcn(pred, true)\n","        # Calculate probability from logits\n","        pred_prob = torch.sigmoid(pred)\n","        # Calculate the modulating factor\n","        p_t = true * pred_prob + (1 - true) * (1 - pred_prob)\n","        # Calculate the alpha factor\n","        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)\n","        modulating_factor = (1.0 - p_t) ** self.gamma\n","        loss *= alpha_factor * modulating_factor\n","\n","        if self.reduction == 'mean':\n","            return loss.mean()\n","        elif self.reduction == 'sum':\n","            return loss.sum()\n","        else:  # 'none'\n","            return loss"],"metadata":{"id":"t3CD6sIxgRPt","executionInfo":{"status":"ok","timestamp":1684409383326,"user_tz":-600,"elapsed":11,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### 7.4 Optimizer and Loss function"],"metadata":{"id":"XhBidDIlvS1e"}},{"cell_type":"markdown","source":["Initialize the optimize, scheduler and loss function."],"metadata":{"id":"15NjJONpiOVa"}},{"cell_type":"code","source":["optimizer = torch.optim.Adam(net_model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,\n","    mode=\"min\",\n","    factor=0.5,\n","    patience=2,\n","    threshold=0.0001,\n","    threshold_mode=\"rel\",\n","    cooldown=0,\n","    min_lr=0,\n","    eps=1e-08,\n","    verbose=False,\n",")\n","loss_function = nn.BCEWithLogitsLoss()\n","# loss_function = FocalLoss()"],"metadata":{"id":"V01bOr-27SYL","executionInfo":{"status":"ok","timestamp":1684409383326,"user_tz":-600,"elapsed":11,"user":{"displayName":"赵涛旭","userId":"02935201968332988464"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["### 7.5 Train"],"metadata":{"id":"Matw6hzs3g9J"}},{"cell_type":"code","source":["tr_loss, tr_precision, tr_recall, tr_f1, vl_loss, vl_precision, vl_recall, vl_f1 = fit(net_model, train_loader,\n","                                                                                       val_loader, MAX_EPOCH,\n","                                                                                       loss_function, optimizer,\n","                                                                                       scheduler)"],"metadata":{"id":"kivPvMTq3jFV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7.6 Plot the Figures of the Results"],"metadata":{"id":"8OJsZsV9vWsl"}},{"cell_type":"markdown","source":["Metrics visualization"],"metadata":{"id":"_70ZRBo0iVW9"}},{"cell_type":"code","source":["plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(tr_loss, label='training_Loss')\n","plt.plot(vl_loss, label='validation_Loss')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Precision\")\n","plt.plot(tr_precision, label='training_Precision')\n","plt.plot(vl_precision, label='validation_Precision')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Recall\")\n","plt.plot(tr_recall, label='training_Recall')\n","plt.plot(vl_recall, label='validation_Recall')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Micro-F1\")\n","plt.plot(tr_f1, label='training_F1')\n","plt.plot(vl_f1, label='validation_F1')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"3PsAtKaqrWq0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8. Inference"],"metadata":{"id":"yZM6WDVh64GU"}},{"cell_type":"markdown","source":["To generate prediction on test data set.\n","\n","Please note that the output $result.csv$ file will be listed in Files in left of this page if this code is run on Colab. You download it from left side.\n","\n","The output file was directly saved in current directory."],"metadata":{"id":"Gg36o9Ebiwv_"}},{"cell_type":"markdown","source":["### 8.1 Predict the Test Data and out to File"],"metadata":{"id":"vnv50TQOvcMy"}},{"cell_type":"code","source":["def predict(model, dataloader):\n","    \"\"\"\n","    Perform predictions using the best trained model.\n","\n","    :param model: trained model\n","    :param dataloader: data loader for prediction\n","    :return: list of image IDs, predicted outputs\n","    \"\"\"\n","    img_id = []\n","    pred = torch.tensor([])\n","    # Set the model to evaluation mode\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader):\n","            img, caption = batch[\"img\"].to(device), torch.from_numpy(np.array(tokenizer(batch[\"caption\"]))).to(device)\n","            # Perform forward pass and apply sigmoid activation\n","            output = nn.Sigmoid()(model(img, caption))\n","            img_id.extend(batch[\"id\"])\n","            # Concatenate predicted outputs along the batch dimension\n","            pred = torch.cat([pred, output.cpu()], dim=0)\n","    # Save predictions to a file\n","    out_to_file(img_id, pred)\n","\n","    return img_id, pred\n","\n","\n","def out_to_file(img_id, prediction):\n","    \"\"\"\n","    Save predictions to a CSV file.\n","\n","    :param img_id: list of image IDs\n","    :param prediction: predicted outputs\n","    :return: None\n","    \"\"\"\n","    label = []\n","    for i, pred in enumerate(prediction):\n","        prediction = []\n","        # Apply thresholding to convert predictions to binary values\n","        pred = pred > THRESHOLD\n","        for idx in range(len(pred)):\n","            if pred[idx]:\n","                if idx > 10:\n","                    # Append the label index (+2) to the prediction list if the index is less than 12\n","                    prediction.append(idx + 2)\n","                else:\n","                    # Append the label index (+1) to the prediction list if the index is larger than 12\n","                    prediction.append(idx + 1)\n","        # Convert the prediction list to a string\n","        result = ' '.join(str(e) for e in prediction)\n","        label.append(result)\n","    for n in range(len(img_id)):\n","        img_id[n] = str(img_id[n])\n","        # Append the file extension to the image ID\n","        img_id[n] = img_id[n]\n","    # Save the DataFrame to a CSV file\n","    df = pd.DataFrame({'ImageID': img_id, 'Labels': label})\n","    df.to_csv(\"Predicted_labels.csv\", index=False)"],"metadata":{"id":"qHuDKVAr7JeN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_pred = Model().to(device)\n","model_pred.load_state_dict(torch.load('best_steps.pth'))\n","\n","predict(model_pred, test_loader)"],"metadata":{"id":"qwYjA-7rsuhU"},"execution_count":null,"outputs":[]}]}